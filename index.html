<!doctype html>
<html>
  <head>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150261217-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-150261217-1');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>StatEcoML Seminar</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  
  <body>
    <div class="wrapper">
      <section>
        <h1 class="title">Stat·Eco·ML Seminar</h1>
        <p class="subtitle"><em>Statistics·Econometrics·Machine Learning Seminar at ENSAE Paris</em></p>

        <ul>
          <li><strong>When:</strong> one talk every two weeks, on Wednesdays from 2pm to 3pm</li>
          <li><strong>Where:</strong> ENSAE Paris, room 3001</li>
          <li><strong>Organizers:</strong> <a href="https://www.linkedin.com/in/martin-mugnier-788310b1/?">Martin Mugnier</a>, <a href="https://francoispierrepaty.github.io">François-Pierre Paty</a>, <a href="https://nicolasschreuder.github.io">Nicolas Schreuder</a></li>
        </ul>

        <p>If you wish to present in the seminar, please register <a href="https://docs.google.com/spreadsheets/d/1ol7ex_IbzXHuTVwhYo99POT_qsC85L76bFqNfnvNshw/edit#gid=0">here</a> and contact the organizers.</p>

        <h2>Upcoming Talks</h2>
        <table>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://vebrunel.com">Victor-Emmanuel Brunel</a></strong><br/>
                (CREST)<br />
                <em>March 4th, 2020</em><br />
                <img src="pictures/victoremmanuel.jpg" alt="Victor-Emmanuel Brunel" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/victor-emmanuel-brunel-steins-method">Stein's method and Berry-Esseen bounds</a></span><br />
                I will present the fundamentals of Stein’s method, based on fairly simple functional equations. This method allows to prove central limit theorems, as well as finite sample approximation bounds such as the well-known Berry-Esseen bounds for normal approximations. It is a very simple yet elegant method which extends far beyond the case of normal approximations for sum of independent variables: It also yields Berry-Esseen type bounds for more general random variables (such as the number of triangles in an Erdös-Rényi graph), as well as finite sample bounds for exponential, or Poisson, or other asymptotic approximations.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://faculty.crest.fr/yguyonvarch/">Yannick Guyonvarch</a></strong><br/>
                (CREST)<br />
                <em>March 18th, 2020</em><br />
                <img src="pictures/yannick.png" alt="Yannick Guyonvarch" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title">On the use of self normalized sums to build non-asymptotic confidence sets</span><br />
                TBA.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://sites.google.com/view/geoffreychinot">Geoffrey Chinot</a></strong><br/>
                (CREST)<br />
                <em>April 1st, 2020</em><br />
                <img src="pictures/geoffrey.jpg" alt="Geoffrey Chinot" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title">High Dimensional linear models: Blessing or curse ?</span><br />
                TBA.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://faculty.crest.fr/xdhaultfoeuille/">Xavier D'Haultfoeuille</a></strong><br/>
                (CREST)<br />
                <em>April 15th, 2020</em><br />
                <img src="pictures/xavier.jpg" alt="Xavier D'Haultfoeuille" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title">TBA.</span><br />
                TBA.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://sites.google.com/view/simonianna">Anna Simoni</a></strong><br/>
                (CNRS)<br />
                <em>April 29th, 2020</em><br />
                <img src="pictures/anna.jpg" alt="Anna Simoni" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title">TBA.</span><br />
                TBA.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://sites.google.com/view/aowang-economics">Ao Wang</a></strong><br/>
                (CREST)<br />
                <em>May 13th, 2020</em><br />
                <img src="pictures/ao.jpg" alt="Ao Wang" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title">On the use of completeness conditions in Econometrics</span><br />
                TBA.
            </td>
          </tr>

        </table>

        

        <h2>Past Talks</h2>
        <table>

           <tr>
            <td class="talk_speaker">
                <strong><a href="https://thomasberrett.github.io">Thomas Berrett</a></strong><br/>
                (CREST)<br />
                <em>February 26th, 2020</em><br />
                <img src="pictures/thomas.jpg" alt="Thomas Berrett" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/thomas-berrett-local-differential-privacy">Local Differential Privacy</a></span><br />
                In recent years, it has become clear that in certain studies there is a need to preserve the privacy of the individuals whose data is collected. As a way of formalising the problem, the framework of differential privacy has prevailed as a natural solution. The privacy of the individuals is protected by randomising their original data before any statistical analysis is carried out and hiding the original data from the statistician. In fact, in local differential privacy, each original data point is only ever seen by the individual it belongs to.<br />
                Research in the area focuses on constructing mechanisms to privatise the data that strike the optimal balance between protecting the privacy of the individuals in the study and allowing the best statistical performance. In many cases it is possible to find minimax rates of convergence under this constraint and thus to quantify the statistical cost of privacy. In this talk I will provide an introduction to the field before presenting some new results.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://echzhen.com">Evgenii Chzhen</a></strong><br/>
                (Orsay)<br />
                <em>February 5th, 2020</em><br />
                <img src="pictures/evgenii.jpg" alt="Evgenii Chzhen" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/evgenii-chzhen-algorithmic-fairness">Algorithmic Fairness in Classification and Regression</a></span><br />
                The goal of this talk is to introduce the audience to the problem of algorithmic fairness. I will provide a general overview on the topic, describe various available frameworks of fairness in classification and regression, and present main approaches to tackle this problem. If time permits, I will present some very recent theoretical results both in classification and regression.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://julesdepersin.github.io">Jules Depersin</a></strong><br/>
                (CREST)<br />
                <em>January 22th, 2020</em><br />
                <img src="pictures/jules.jpg" alt="Jules Depersin" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/jules-depersin-robust-fast-estimation">Robust and Fast Estimation for Heavy-Tailed Distributions</a></span><br />
                When it comes to estimating the mean of a heavy-tailed distribution (or in the presence of outliers), the empirical mean does not give satisfying results. This issue has been dealt with using tools such as Median-Of-Mean (MOM) estimators. Such estimators are very simple to compute and give optimal rates of convergence when the dimension of the random variable is small, but fail to do so in high-dimensional set-ups. We will try to explain why, giving simple exemples and intuitions, and we will introduce tools needed to study high dimensions.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://www.linkedin.com/in/julien-chhor-268547150">Julien Chhor</a></strong><br/>
                (CREST)<br />
                <em>January 8th, 2020</em><br />
                <!--<img src="pictures/julien.jpg" alt="Julien Chhor" class="picture" />-->
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/julien-chhor-minimax-testing-in-random-graphs">Minimax Testing in Random Graphs</a></span><br />
                In a lot of recent statistical applications, the intensifying use of networks has made large random graphs a decisive field of interest. To name a few topics, we can mention community detection (in the stochastic block model or in social networks), as well as network modelling, or in modelling the brain. On the other hand, the existing literature about hypothesis testing is profuse. Yet quite surprisingly, only little literature exists about hypothesis testing in random graphs. In this talk, we fill the gap by studying two different testing problems in inhomogeneous Erdös-Rényi random graphs. After having introduced general tools for minimax testing, we first study a two sample testing problem in random graphs under sparsity constraints and second, the goodness-of-fit problem (also called identity testing problem), for which we identify minimax-optimal adaptive tests.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://tlacombe.github.io">Théo Lacombe</a></strong><br/>
                (INRIA Saclay)<br />
                <em>December 4th, 2019</em><br />
                <img src="pictures/theo.jpg" alt="Théo Lacombe" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/theo-lacombe-inria-an-introduction-to-tda">An Introduction to Topological Data Analysis</a></span><br />
                Topological Data Analysis (TDA) is a recent approach in Data Sciences that aims to encode some structured objects---think of graphs, time series, points on a manifold for instance---with respect to the topological information they contain.<br />
                The first half of this introductive lecture will give a high-level picture of TDA. <br />
                We will then briefly introduce the persistent homology, a notion coming from algebraic topology that is central in TDA to build our topological signatures.<br />
                Finally, the last part of the talk will present some statistical and learning aspects of TDA.
            </td>
          </tr>

          <tr>
            <td class="talk_speaker">
                <strong><a href="https://francoispierrepaty.github.io">François-Pierre Paty</a></strong><br/>
                (CREST)<br />
                <em>November 20th, 2019</em><br />
                <img src="pictures/francoispierre.jpg" alt="François-Pierre Paty" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/francois-pierre-patycrest-an-introduction-to-optimal-transport">An Introduction to Optimal Transport</a></span><br />
                Optimal transport (OT) dates back to the end of the 18th century, when French mathematician Gaspard Monge proposed to solve the problem of <em>déblais</em> and <em>remblais</em>. Yet, the mathematical formulation of Monge was rapidly found to meet its limits in the lack of provable existence of the studied objects. It is only after 150 years that OT enjoyed a resurgence, when Kantorovich understood the suitable framework that would allow to solve Monge’s problem and give rise to fundamental tools and theories in probability, optimization, differential equations and geometry. While applications in economics have a long history, it has only been recently that OT has been applied to statistics and machine learning, as a way to analyze data. In this mini-lecture, I will first define OT and present the most prominent results of OT theory. Then, I will give an overview of the current research in statistical and algorithmic OT, with an emphasis on machine learning and economics applications.
            </td>
          </tr>
          
          <tr>
            <td class="talk_speaker">
                <strong><a href="https://badreddinecheriefabdellatif.github.io">Badr-Eddine<br/>Chérief-Abdellatif</a></strong><br/>
                (CREST)<br />
                <em>November 6th, 2019</em><br />
                <img src="pictures/badr.jpg" alt="Badr-Eddine Chérief-Abdellatif" class="picture" />
            </td>
            <td class="talk_abstract">
                <span class="talk_title"><a href="http://crest.science/event/badr-eddine-cherief-abdellatif">Theoretical Study of Variational Inference</a></span><br />
                Bayesian inference provides an attractive learning framework to analyze and to sequentially update knowledge on streaming data, but is rarely computationally feasible in practice. In the recent years, variational inference (VI) has become more and more popular for approximating intractable posterior distributions in Bayesian statistics and machine learning. Nevertheless, despite promising results in real-life applications, only little attention has been put in the literature towards the theoretical properties of VI. In this talk, we aim to present some recent advances in theory of VI. First, we show that variational inference is consistent under mild conditions and retains the same properties than exact Bayesian inference in the batch setting. Then, we study several online VI algorithms that are inspired from sequential optimization in order to compute the variational approximations in an online fashion. We provide theoretical guarantees by deriving generalization bounds and we present empirical evidence in support of this.
            </td>
          </tr>

        </table>



      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
